/**
 * Migration script generation: per-collection migrate scripts and run-all runner.
 */

import type {
  NoSqlCollection,
  NoSqlField,
  SqlForeignKey,
  SqlTable,
} from "@s2n/core";
import { pascalCase } from "./utils";

export function generateMigrationScriptForCollection(
  collection: NoSqlCollection,
  dependencies: string[],
  sqlTable?: SqlTable,
  tableFks: SqlForeignKey[] = [],
): string {
  const funcName = `build${pascalCase(collection.name)}Doc`;
  const objectLiteral = renderNoSqlFieldsMapping(collection.fields ?? []);
  const tableName = sqlTable?.name ?? collection.name;
  const primaryKeyFields = sqlTable?.primaryKey ?? [];
  const singlePkField =
    primaryKeyFields.length === 1
      ? primaryKeyFields[0]
      : primaryKeyFields[0] ??
        collection.fields?.find((f) => /id$/i.test(f.name))?.name ??
        "_id";
  const isCompositePk = primaryKeyFields.length > 1;

  const relatedLoadCode =
    dependencies.length > 0
      ? `
  // Load related data from MongoDB (dependencies: ${dependencies.join(", ")})
  const related = {};
${dependencies
  .map(
    (dep) => `  const ${dep}Collection = mongoDb.collection("${dep}");
  const ${dep}Map = new Map();
  const ${dep}Docs = await ${dep}Collection.find({}).toArray();
  for (const doc of ${dep}Docs) {
    const depId = doc.${pascalCase(dep)}Id ?? doc.${dep.toLowerCase()}id ?? doc._id;
    if (depId != null) {
      ${dep}Map.set(depId, doc);
    }
  }
  related.${dep} = ${dep}Map;`,
  )
  .join("\n")}`
      : `  const related = {};`;

  const relatedLookupCode = dependencies
    .map((dep) => {
      const fkField =
        collection.fields?.find(
          (f) =>
            f.name.toLowerCase() === `${dep.toLowerCase()}id` ||
            f.name.toLowerCase() === `${dep.toLowerCase()}_id`,
        )?.name ?? null;
      if (!fkField) return null;
      return `      const ${dep}Id = row["${fkField}"] ?? row["${pascalCase(fkField)}"];
      const ${dep}Doc = ${dep}Id != null ? related.${dep}.get(${dep}Id) : undefined;
      if (${dep}Doc) relatedData.${dep} = ${dep}Doc;`;
    })
    .filter(Boolean)
    .join("\n");

  const indexCreationLines: string[] = [];
  if (primaryKeyFields.length > 0) {
    const pkKeys = primaryKeyFields.map((f) => `${f}: 1`).join(", ");
    indexCreationLines.push(
      `    await collection.createIndex({ ${pkKeys} }, { unique: true });`,
    );
  }
  const pkSet = new Set(primaryKeyFields);
  for (const fk of tableFks) {
    if (pkSet.has(fk.fromColumn)) continue;
    indexCreationLines.push(
      `    await collection.createIndex({ ${fk.fromColumn}: 1 });`,
    );
  }
  const indexCreationCode =
    indexCreationLines.length > 0
      ? `    // Index-first (PK and FK) for production readiness
${indexCreationLines.join("\n")}
`
      : "";

  const orderByClause =
    primaryKeyFields.length > 0
      ? ` ORDER BY ${primaryKeyFields.map((c) => `"${c}"`).join(", ")}`
      : "";

  const compositePkFilterBuild =
    isCompositePk && primaryKeyFields.length > 0
      ? primaryKeyFields
          .map(
            (f) =>
              `      const _pk_${f} = doc["${f}"] ?? doc["${pascalCase(f)}"];`,
          )
          .join("\n") +
        "\n      const _filter = { " +
        primaryKeyFields.map((f) => `${f}: _pk_${f}`).join(", ") +
        " };\n      if (" +
        primaryKeyFields.map((f) => `_pk_${f} == null`).join(" || ") +
        ") { console.warn(\`Row missing composite PK, skipping:\`, row); continue; }"
      : "";

  const singlePkFilterBuild =
    !isCompositePk
      ? `      const docId = doc["${singlePkField}"] ?? doc["${pascalCase(singlePkField)}"] ?? doc._id;
      if (docId == null) {
        console.warn(\`Row missing ID field "${singlePkField}", skipping:\`, row);
        continue;
      }
      const _filter = { ${singlePkField}: docId };`
      : "";

  // Below: template for the emitted .migrate.js file (the // lines are output, not dead code)
  return `// Auto-generated by sql2nosql. Edit as needed.
// Migrates Postgres table "${tableName}" -> MongoDB collection "${collection.name}".
// Dependencies: ${dependencies.length > 0 ? dependencies.join(", ") : "none"}
// Run: node ${collection.name}.migrate.js

const path = require("path");
const fs = require("fs");
const { Client: PgClient } = require("pg");
const { MongoClient } = require("mongodb");

// --- Safe to edit: buildDoc and custom logic below ---
function ${funcName}(row, related = {}) {
  return ${objectLiteral};
}
// --- END safe edit ---

async function migrate() {
  const configPath = path.resolve(__dirname, "..", "..", "..", "..", "sql2nosql.config.json");
  if (!fs.existsSync(configPath)) {
    throw new Error(\`Config file not found at \${configPath}\`);
  }
  const config = JSON.parse(fs.readFileSync(configPath, "utf8"));
  const pgConnection = config.connection;
  const schema = config.schema || "public";
  const mongodbConfig = config.mongodb || {};
  const mongoUri = mongodbConfig.uri || "mongodb://localhost:27017";
  const mongoDbName = mongodbConfig.database || "sql2nosql";
  const collectionPrefix = mongodbConfig.collectionPrefix || "";
  const migrationConfig = config.migration || {};
  const batchSize = migrationConfig.batchSize || 0;
  const dryRun = migrationConfig.dryRun === true;
  const skipOnError = migrationConfig.skipOnError === true;
  const progressEvery = migrationConfig.progressEvery || 1000;

  if (!pgConnection) {
    throw new Error("Missing Postgres connection in sql2nosql.config.json");
  }
  if (!dryRun && !mongoUri) {
    throw new Error("Missing mongodb.uri in sql2nosql.config.json (or set migration.dryRun to true)");
  }

  console.log(\`Migrating ${collection.name} from Postgres to MongoDB\${dryRun ? " (dry-run, no writes)" : ""}...\`);

  const pg = new PgClient({ connectionString: pgConnection });
  const mongoClient = new MongoClient(mongoUri);

  try {
    await pg.connect();
    await mongoClient.connect();
    const mongoDb = mongoClient.db(mongoDbName);
    const collection = mongoDb.collection(\`\${collectionPrefix}${collection.name}\`);

    // --- BEGIN GENERATED ---
${relatedLoadCode}
${indexCreationCode}
    const orderBy = "${orderByClause.replace(/"/g, '\\"')}";
    let totalRows = 0;
    let migrated = 0;
    let errors = 0;
    let offset = 0;
    const limit = batchSize > 0 ? batchSize : 2147483647;

    while (true) {
      const query = \`SELECT * FROM "\${schema}"."${tableName}"\${orderBy} LIMIT \${limit} OFFSET \${offset}\`;
      if (offset === 0) console.log(\`Querying: \${query}\`);
      const res = await pg.query(query);
      if (res.rows.length === 0) break;
      totalRows += res.rows.length;

      for (const row of res.rows) {
        const relatedData = {};
${relatedLookupCode}

        const doc = ${funcName}(row, relatedData);
        if (!doc) continue;

${compositePkFilterBuild}
${singlePkFilterBuild}

        try {
          if (!dryRun) {
            await collection.updateOne(
              _filter,
              { $set: doc },
              { upsert: true },
            );
          }
          migrated++;
          if (progressEvery > 0 && migrated % progressEvery === 0) {
            console.log(\`  ... \${migrated} rows migrated\`);
          }
        } catch (err) {
          if (skipOnError) {
            errors++;
            console.error(\`Row error (\${errors}):\`, _filter, err.message);
          } else {
            throw err;
          }
        }
      }
      if (res.rows.length < limit) break;
      offset += limit;
    }

    console.log(\`âœ“ Migrated \${migrated} documents into "\${collectionPrefix}${collection.name}"\${dryRun ? " (dry-run)" : ""}. Total rows: \${totalRows}\${errors > 0 ? \`, errors: \${errors}\` : ""}\`);
    // --- END GENERATED ---
  } finally {
    await pg.end().catch(() => {});
    await mongoClient.close().catch(() => {});
  }
}

if (require.main === module) {
  migrate().catch((err) => {
    console.error("Migration failed:", err);
    process.exit(1);
  });
}

module.exports = { ${funcName}, migrate };
`;
}

function renderNoSqlFieldsMapping(
  fields: NoSqlField[],
  indent: number = 2,
  parentPath?: string,
): string {
  const pad = " ".repeat(indent);
  const lines: string[] = ["{"];

  for (const field of fields) {
    if (field.type === "object" && field.fields && field.fields.length > 0) {
      const nestedPath = parentPath ? `${parentPath}.${field.name}` : field.name;
      const nestedObject = renderNoSqlFieldsMapping(
        field.fields,
        indent + 2,
        nestedPath,
      );
      lines.push(`${pad}${field.name}: ${nestedObject},`);
    } else {
      const expr = parentPath
        ? `related.${parentPath} && related.${parentPath}["${field.name}"]`
        : `row["${field.name}"]`;
      lines.push(`${pad}${field.name}: ${expr},`);
    }
  }

  lines.push(`${" ".repeat(Math.max(indent - 2, 0))}}`);
  return lines.join("\n");
}

export function generateMigrationRunnerScript(
  collections: NoSqlCollection[],
): string {
  type CollectionInfo = {
    name: string;
    moduleVar: string;
    funcName: string;
    deps: Set<string>;
  };

  const allNames = new Set(collections.map((c) => c.name));

  function inferDeps(collection: NoSqlCollection): Set<string> {
    const deps = new Set<string>();

    function visitFields(fields: NoSqlField[]) {
      for (const field of fields) {
        if (field.type === "object") {
          const candidates = new Set<string>();
          candidates.add(field.name);
          candidates.add(`${field.name}s`);
          if (field.name.endsWith("s")) {
            candidates.add(field.name.slice(0, -1));
          }

          for (const cand of candidates) {
            if (allNames.has(cand) && cand !== collection.name) {
              deps.add(cand);
            }
          }

          if (field.fields) {
            visitFields(field.fields);
          }
        }
      }
    }

    visitFields(collection.fields ?? []);
    return deps;
  }

  const infos: CollectionInfo[] = collections.map((c) => ({
    name: c.name,
    moduleVar: pascalCase(c.name),
    funcName: `build${pascalCase(c.name)}Doc`,
    deps: inferDeps(c),
  }));

  const ordered: CollectionInfo[] = [];
  const tempMark = new Set<string>();
  const permMark = new Set<string>();

  function visit(name: string) {
    if (permMark.has(name)) return;
    if (tempMark.has(name)) {
      return;
    }
    tempMark.add(name);
    const info = infos.find((i) => i.name === name);
    if (info) {
      for (const dep of info.deps) {
        visit(dep);
      }
      permMark.add(name);
      tempMark.delete(name);
      ordered.push(info);
    }
  }

  for (const info of infos) {
    visit(info.name);
  }

  const imports = ordered
    .map((info) => {
      const file = `${info.name}.migrate.js`;
      return `const ${info.moduleVar} = require("./${file}");`;
    })
    .join("\n");

  const listEntries = ordered
    .map(
      (info) =>
        `  { name: "${info.name}", buildDoc: ${info.moduleVar}.${info.funcName} },`,
    )
    .join("\n");

  // Emitted run-all.migrate.js source (leading // in the string are output comments)
  return (
    `// Auto-generated by sql2nosql.\n` +
    `// This is a placeholder runner that wires all per-collection migration scripts.\n` +
    `// You are expected to:\n` +
    `// 1) Connect to Postgres and MongoDB\n` +
    `// 2) Stream rows from Postgres\n` +
    `// 3) Call the appropriate buildDoc(row, related) and upsert into MongoDB.\n\n` +
    `${imports}\n\n` +
    `async function runAllMigrations() {\n` +
    `  const scripts = [\n${listEntries}\n  ];\n` +
    `  console.log("Loaded migration scripts:", scripts.map((s) => s.name));\n` +
    `  // TODO: implement database connections and actual upsert logic here.\n` +
    `}\n\n` +
    `module.exports = { runAllMigrations };\n`
  );
}
